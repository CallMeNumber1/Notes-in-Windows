#### 无监督学习

----

聚类算法clustering：给定一组未加标签的数据集，将数据集分成不同的簇

​	可用于市场划分、社交网络分析、组织计算机集群等

**K-means**

迭代的过程

聚类中心

1.簇分配

2.移动聚类中心

- `input`:K；Training set

  $xi$是n维向量而非n+1维了

- 随机初始化K个聚类中心$\mu_1,...,\mu_k$

- 簇分配

- 移动聚类中心

如果出现一个没有点的聚类中心，直接这个聚类中心

当用于没那么明显分开的数据（比如T恤制造）类似于市场分割

**优化目标**

了解这个有助于我们调试K均值算法，防止陷入局部最优

$c^{(i)}$第i个样例归属的簇的索引

$\mu_k$第k个聚类中心
$$

$$
此代价函数也叫做K均值算法的失真

算法的第一步簇分配，实际上就是在最小化失真函数J，它的参数是$c^{(i)}$，保持聚类中心的未知不变。

第二步移动聚类中心，实际上是选择$\mu_k $来最小化代价函数J

**随机初始化**

使算法避开局部最优

$K<m$

随机选择K个训练样例，令$\mu_1,...,\mu_k$等于这些训练样例



随机初始化状态不同，最后收敛结果可能不同

局部最优值指的是K均值算法落在局部最优，因而不能很好的最小化失真函数J

为了避免得到局部最优，我们随机初始化多次来运行多次K均值算法（例如50-1000），然后我们可以用我们得到的聚类分配和聚类中心计算代价函数J，然后选择100种分类数据的方法种代价函数最小的一种。

当K=2-10时，多次随机初始化能保证得到较好的结果

当K很大时，多次随机初始化可能不会改善太多

**选取聚类数量**

实际上最常见的方法是手动选择

- 1.肘部法则（不是每次都有效果）

一开始畸变值下降的很快，到这个点之后下降的变慢

但实际运用种不一定会有一个清晰的拐点

- 2.看哪个聚类数量能更好地应用于后续目的

#### 降维

----

降维的应用1：压缩数据。

可以减少对内存的需求，可以加快算法的运行速度

应用2：可视化数据，更好地理解数据

**PCA**

做PCA前要先进行均值归一化，特征缩放

- 要从二维降到一维，试图找到一个向量$u^{1} \in R^n$，将数据投影到上面来最小化投影误差

  向量的正负没有关系。

- 要从n维降到k维，试图找到k个向量，将数据投影到这k个向量展开的线性子空间上。

   可以想成是寻找k个方向，而不只是一个方向来对数据进行投影。

找出最小化投影距离的方式来对数据进行投影

PCA不是线性回归：线性回归最小化的时点与函数值之间的距离，而PCA最小化的是投影距离。并且PCA所有特征同等对待，没有特别的要预测的y

PCA所做的：试图找到一个低维的平面对数据进行投影，以便最小化投影误差的平方，以及最小化每个点与投影后的对应点之间距离的平方值。

**PCA2**

- 先进行均值归一化和特征缩放
- 之后找到要投影的线性子空间$u^{(1)},...,u^{(k)}$
- 再找到原始数据集的低维表示：$x \in R^n$---->$z \in R^k$

协方差矩阵$\Sigma​$（$n \times n​$）
$$
\Sigma = \frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T奇异值分解svd
$$
奇异值分解svd

```matlab
[U, S, V] = svd(Sigma);			% 使用eig()也可以
```

- $U$为$n \times n$矩阵，提取前k列，给了我们k个方向，即为我们想投影数据的方向

  取前k列即得到$U_{reduce}​$（$n \times k​$）

- $z^{(i)} = U_{reduce}^T \times x^{(i)}$，得到$z^{(i)}$为k维向量

类似于K-means算法，$x \in R^n$（即$x_0 \neq 1$）

**主成分数量选择**

- 平均均方误差：$\frac{1}{m}\sum_{i=1}^m||x^{(i)} - x_{approx}^{(i)}||^2​$
- 训练集方差：$\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2​$

```matlab
[U, S, V] = svd(Sigma);			% 使用eig()也可以
```

- S为一个对角阵($n \times n $)，仅对角线上元素非0，其他全0，我们可以用这个矩阵计算上面两个数值的比例。

$$
\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)} - x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2} = 1 - \frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^mS_{ii}} \le 1\%
$$

选择一个最小的k值满足上面公式即可。下面是一个更好的计算方式：

在调用一次SVD之后，只需要找到一个最小的k并且满足下式即可。
$$
\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^mS_{ii}} \ge 99\%
$$
**压缩重现**

由被压缩的数据来得到原来数据x的近似，这一过程也叫原始过程的重构。

$z \in R​$ --> $x \in R^2​$
$$
x_{apporx}^{(1)} = U_{reduce}z^{(1)},得到的x_{approx} \approx x
$$
**应用PCA的建议**

可以用PCA对监督学习进行加速

注意：PCA仅仅在训练数据上运行，找到一个从x到z的映射。可以将这个映射再运用到交叉验证集和测试集上。

- 压缩

  减少内存需求

  加速学习算法：根据保留方差的百分比，选择合适的维度k

- 可视化：将数据变为2D或3D来可视化

注1：PCA不是一种防止过拟合的好方法，相对于正则化，PCA减少特征时没有考虑到标签y，更有可能丢失一些有价值的信息。即使方差保留了99%或95%，使用正则化会更好。

注2：不要在项目的一开始就将PCA考虑在内，先看原始数据$x^{(i)}$可以做什么，这么做不能达到目标之后再去考虑使用PCA，在学习算法运行缓慢或内存硬盘空间不足时再去考虑应用PCA，而不是在做计划时首先就花了大量时间应用PCA，选择k等。