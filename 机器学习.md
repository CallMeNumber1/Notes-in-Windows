## 机器学习西瓜书

#### 第一章-绪论

机器学习提供数据分析能力，云计算提供数据处理能力，众包提供数据标记能力

机器学习领域和数据库领域是数据挖掘的两大支撑

自动驾驶汽车中，机器学习起到了“司机”的作用

#### 第二章-模型评估与选择

**经验误差与过拟合**

误差均指误差期望

经验误差：学习器在训练集上的误差

泛化误差：在新样本上的误差

过拟合是机器学习面临的关键障碍，无法彻底避免，能做的只是缓解或减少其风险。

**评估方法**

使用一个`测试集`来测试学习器对新样本的判别能力，然后以测试集上的`测试误差`作为泛化误差的近似。

1. 留出法：直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T

2. 交叉验证法

   k=样本数量时，即特例，为留一法，但计算复杂度比较高

3. 自助法

   以自助采样法（有放回采样）为基础

   在数据集较、难以有效划分训练集/测试集时很有用

   产生的数据集改变了初始数据集的分布，会引入估计偏差

4. 调参与最终模型

   要注意，通常把学得模型在实际使用中遇到的数据称为`测试数据`，为了加以区分，模型评估与选择中用于评估测试的数据集常称为`验证集`。

   :question: 我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。

   个人理解：即训练集、验证集、测试集？

**性能度量**

1. 错误率与精度

2. 查准率、查全率与F1

   查准率和查全率是一对矛盾的度量，查准率高时，查全率往往偏低。

   查准率：挑出的瓜中有多少是好瓜。查全率：所有好瓜中有多少比例被挑了出来

   P-R曲线：以查准率为纵轴，查全率为横轴作图，就得到查准率-查全率曲线

   ​	若一个学习器的P-R曲线被另一个学习器的曲线完全”包住“，则说明不如后者优越。

   ​	若两个学习器的曲线发生了交叉，则一般很难断言两者孰优孰劣

   综合考虑查准率、查全率的性能度量

   ​	1.平衡点（BEP）

   ​	2.F1度量

3. ROC与AUC

4. 代价敏感错误率与代价曲线

   回归前面介绍的一些性能度量可看出，它们大都隐式地假设了均等代价，比如计算错误次数，并没有考虑不同错误会造成不同的后果。

   在非均等代价下，我们希望的不再是简单地最小化错误次数，而是希望最小化“总体代价”

**比较检验**

1. 假设检验（关于单个学习器泛化性能的假设进行检验）

   根据测试错误率估推出泛化错误率的分布

2. 交叉验证t检验

**偏差与方差**

解释学习算法泛化性能的一种重要工具

泛化误差可以分解为偏差、方差、噪声之和

​	偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力

​	方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响

​	噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度

偏差-方差分解说明：泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

？：t检验 $x^2$检验

#### 第三章 线性模型



## 吴恩达机器学习

#### 第一章

**监督学习**

给算法一个数据集，其中包含了”正确答案“

1. 回归问题

​	目标是预测一个连续值输出

2. 分类问题（两类、多类）

​	预测离散值输出

如何处理无穷多个特征/属性

**无监督学习**

给出的数据没有任何标签

1. 聚类算法

​	簇（cluster）

#### 第二章

**模型描述**

线性回归(linear regression)

向学习算法提供训练集，输出一个函数，通常用小写h表示**`假设函数`**

**代价函数**

平方误差代价函数（用于回归问题）

$minimize(\theta_0, \theta_1)$ $J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}{(h(x) - y)}^2​$

**梯度下降**

$\theta_j := \theta_j - \alpha \frac{\partial(\theta_0, \theta_1)}{\partial\theta_j}​$

$\alpha​$过小会导致收敛缓慢，过大会导致不

当到达局部最低点时，梯度下降相当于什么都没做，于我们的预期相符

当我们接近局部最低点时，导数值会越来越小，梯度下降将自动采取较小的幅度

可以尝试用它最小化任何代价函数，而不仅是线性回归中的代价函数 

**梯度下降应用到线性回归问题上---线性回归的梯度下降**

Batch梯度下降法

​	每一次梯度下降，都遍历了整个训练集的样本

另一种求解代价函数J最小值的方法：正规方程组法，之后介绍

#### 多变量线性回归

**多特征**

特征向量

​	额外定义一个$x_0 = 1​$，则$h(x) = \theta x_0 + \theta x_1 + ... + \theta x_n​$，，进一步可以写为

​	$h(x) = \theta ^Tx​$

符号定义

​	:boom:$x^{(i)}​$为第i个样本

​	$x_1​$为第一个特征

**多元梯度下降法**

**多元梯度下降法演练I：特征缩放**

特征值的范围要接近，这样收敛过程会变快

并不需要太精确，只是为了让梯度下降运行的更快一点

1. 除以范围

2. 均值归一化

   $x1 = \frac{x_1 - \mu_1}{s1}，\mu_1为均值，s1为范围（或标准差）​$

**多元梯度下降法演练II：学习率**

确定梯度下降正确运行：

1. 通过绘制$J(\theta)$随迭代步数变化的曲线图观察，迭代之后代价函数变小(倾向于)

   如果曲线是上升的，往往可以通过减小学习率$\alpha​$来实现

   如果曲线循环往复上升下降，也可以减小学习率

2. 自动收敛测试

   如果一步迭代后的下降小于1e-3，则说明已经收敛

学习率太小时，收敛(convergence)会很慢

学习率太大时，$J(\theta)$可能不会收敛

​	尝试不同的$\alpha$值：0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1

**特征和多项式回归**

定义新特征有时候可能得到更好的模型

如何将一个多项式（如一个二次函数/三次函数）拟合到我们的数据上

​	$h_0(x) = \theta_0 + \theta_1(size) + \theta_2(size)^2 + \theta_3(size)^3$

​	令$x_1 = (size), x_2 = (size)^2, x_3=(size)^3$

​	则$h_0(x) = \theta _0 + \theta x_1 + \theta x_2 + \theta x_3$

**正规方程**

$\theta = (X^{T}X)^{-1}X^{T}y​$

得到的 $\theta$ 即为使得代价函数最小的值

使用正规方程求解时可以不必进行特征缩放

**梯度下降与正规方程的选择**

梯度下降法需要选择合适的学习速率 $\alpha​$ ，并且需要很多迭代，还要画图判断敛散性，而正规方程不需要。

然而当n很大时，正规方程的速度会很慢，求解$(X^{T}X)^{-1}$的时间接近于$O(n^3)$，因此当n小于10000时，可以选择正规方程求解，当大得多时，就要考虑梯度下降或者其他方法了

#### Logistic回归

预测离散值

**分类**

不推荐将线性回归用于分类问题

​	因为对于数据集中y=0，1，而实际$h_\theta (x)​$可能大于1或者小于0

logistic回归实际上是一种分类算法

​	0<= h(x) <= 1

**假设表示** 

`sigmoid函数`

由$h_\theta(x) =g(\theta^Tx)$   $g(z) = \frac{1}{1 + e^{-z}}$

得到$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}​$

$P(y = 0|x;\theta)​$ ：给定参数$\theta​$，给定特征x的条件下y=0的概率

$h_\theta(x) = P(y = 1|x;\theta)​$

**决策边界**

是假设函数的一个属性，而不是训练集的属性，只要给定了$\theta​$就确定

 $\theta^Tx = 0$时

特征也可以是多项式的

**代价函数**

若将假设函数代入之前的代价函数，则代价函数成为一个非凸函数（因为logistic回归中$h_\theta(x)$是非线性的），当应用梯度下降时可能进入局部最优

因此定义代价函数$J(\theta)$为：
$$
J(\theta) = \frac{1}{m}Cost(h_\theta(x), y^{(i)})\\
单个样本的代价：
Cost(h_\theta(x), y) = \begin{cases}
-log(h_\theta(x)) & y = 1 \\
-log(1-h_\theta(x)) &  y = 0 
\end{cases}
$$
当y=1,h(x)=1时，假设函数预测正确，代价值为0，

当y=1,h(x)->0时（即当y=1时，假设函数错误的认为y=1），cost->$\infin​$，代价趋于无穷，即给一个很大的惩罚值

**简化的代价函数与梯度下降**
$$
Cost = -ylog(h_\theta(x)) - (1 - y)log(1-h_\theta(x)) \\
J(\theta) = -\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)}) + (1 - y^{(i)})log(1-h_\theta(x^{(i)}))]
$$
此代价函数是从统计学中的极大似然估计得来的，同时具有一个很好的性质--凸函数。

因此是大部分人用来拟合logistic回归模型的代价函数

要求$minJ(\theta)​$：
$$
重复进行
\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta)\\
即:\theta_j := \theta_j - \alpha\sum_{i = 1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
$$
梯度下降的方式形式和线性回归时相同，然而$h_\theta(x)​$不同，导致不同

同样的，可以使用`特征缩放`来加快梯度下降收敛的速度

**高级优化**

比梯度下降快得多得算法比如：

​	共轭下降

​	...

优化算法库会使程序变得模糊，难于调试，但会加快梯度下降的速度

**多元分类**：一对多(one-vs-all)

将多个类的分类变为多个二元分类

#### 正则化

**过拟合问题**

千方百计拟合训练集，导致无法泛化到新的样本中

应对方法：

​	1.减少特征数量

​	2.正则化：保留所有特征

**代价函数**
$$
J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}{(h_\theta(x)^{(i)} - y^{(i)})}^2 + \lambda\sum_{j=1}^n\theta_j^2]
$$
正则化参数：控制两个不同目标之间的取舍

​	1.想要更好的拟合数据

​	2.保持参数尽量小（从而保持模型尽量简单，避免过拟合的发生）

如果正则化参数设置太大，会发生欠拟合

**线性回归的正则化**

使用梯度下降方法时

使用正规方程时

利用它可以避免过拟合的问题，这样可以在较小的训练集而特征较多的情况下使用

**Logistic回归的正则化**



#### 神经网络学习

**非线性假设**

因为当特征很多时，几乎不可能使用线性假设

神经元与大脑

神经网络再次兴起的原因，计算能力的提升

**模型展示I**

第一层为输入层（输入特征），最后一层为输出层（输出假设的最终计算结果），中间的若干层为隐藏层

​	输入层必要的时候会加一个$x_0​$（为1），称为偏置单元

带有sigmoid函数的激活函数

$\theta​$称为模型的参数/权重

神经网络是一组神经单元组成的集合

如果神经网络在层次j上有$s_j$个单元，在层次j+1上有$s_{j+1}$个单元，则$\theta^{(j)}$控制j到j+1层之间映射的矩阵，维度为 ​$s_{j+1}*(s_j + 1)$ 

a的意义：

**模型展示II**

前向传播的工作原理

​	从输入层的激活项开始，前向传播到第一隐藏层，然后传播到第二隐藏层，最终到达输出层。

向量化计算h(x)

神经网络利用隐藏层计算更复杂的特征，并输入到最后的输出层，从而学习更复杂的非线性假设函数

**例子**

神经网络如何计算复杂的非线性假设

为何神经网络可以计算复杂的函数

 当网络中有很多层，第二层有一些关于输入的简单函数，第三层在此基础上，计算更复杂的方程，再往后一层，计算的函数越来越复杂

视频种以手写邮编的识别为例

#### **反向传播**

L表示神经网络的层数

**代价函数**

重点讲解神经网络在分类问题中的应用

- 二元分类

最后输出层只有一个单元，输出的$h_\Theta(x)$ 是一个实数

即K=1（K为输出单元的个数）

- 多元分类

我们的假设会输出K维向量，$h_\Theta(x) \in R^K$输出单元的个数也等于K

同时可知$K\ge3$（否则就为二元分类了）

正则化时不会将偏差项正则化，正则化的也没有大影响

$\delta^{(L)} = a^{(L)}-y^{(i)}$，不计算$\delta^{(1)}$，因为是输入层，不存在误差

------

#### Octave/Matlab基本操作

ones(n)生成一个全1的n阶方阵

eye(n)生成单位矩阵

hist()作图

size(A): 返回一个矩阵，记录A的两个维度行和列

length(a)：计算向量的长度，或者计算矩阵中最长的一维

C = [A B]：连接A、B（左右）

C = [A; B]: 连接A、B（上下）

A(:)：返回一个列向量

A(:, 2)：返回第二列的全部元素，也可以作为左值，给第二列赋值

load xxx.dat 加载数据

save hello.mat v 储存数据

点`.`说明是针对矩阵中元素的运算

​	如A .* B、A.^2、1 ./ A（矩阵每个元素取倒数）

常用函数 v = [1; 2; 3]

​	log(v)、exp(v)、v + ones(3, 1)相当于v + 1、

​	`A'`：A的转置

​	val = max(a)、[val, ind] = max(a) （其中a是一个向量，否则会得到每一列的最大值）

​	rand(3) magic(3)

​	max(A, [], 1) max(A, [], 2)

​	max(max(A)) = max(A(:))（求解矩阵中的最大元素，先变为一个向量）	

​	sum(A, 1) sum(A, 2) prod(a)

​	flipud(A)使矩阵垂直翻转（一般来取单位矩阵的反对角线）

​	pinv(A)求矩阵的逆矩阵（伪逆矩阵）

​	imagsc(A) 可视化矩阵A

**可视化**

hold on; 在原图基础上继续作图

plot(t, y1, 'r') 第三个参数可选，指定颜色

xlabel('time') ylabel('value') legend('sin', 'cos') title('myplot')

subplot(1, 2, 1) 把图分成1*2的网格，当前使用第一个

保存为图片：print -dpng 'myplot.png'

**控制语句、函数**

```matlab
% for
for i = indices,

	....

end;
for i = 1:n + 1,
	...
end;
% if
if v(i) == 1,
	...
elseif v(i) == 2,
	...
else 
	...
end;
% function 1
function y = square(x)
y = x^2;
% function 2
function [y1, y2] = squareAndCube(x)
y1 = x^2;
y2 = x^3;
%
[a, b] = squareAndCube(x)
```

**向量化**

能得到高效得多的线性回归算法，不仅编写效率更高，运行效率也更高

比如：$h(X) = \theta^TX​$

:question: 向量化计算$\theta$

## 线性代数

**矩阵和向量**

**加法和标量乘法**

**矩阵向量乘法**

​	解决预测问题，一行代码，让数据矩阵和参数矩阵相乘

**矩阵乘法**

可以将大量的运算打包成一次矩阵乘法

**矩阵乘法特征**

​	不满足交换律

​	满足结合律 $A\times (B \times C) = (A\times B) \times C$

​	单位矩阵相当于实数范围内1的作用

**矩阵的逆**inverse

​	方阵才有逆矩阵

​	$AA^{-1} = A^{-1}A =  I$

​	如果A中所有元素为0，则其没有逆矩阵

​	没有逆矩阵的矩阵也称为`奇异矩阵`或`退化矩阵`（可看成非常接近于0）

**矩阵的转置**transpose

$A^T$

## 所遇到相关知识

**西瓜书**

独立同分布

概率密度函数

**吴恩达**

什么样的矩阵有逆矩阵

偏导