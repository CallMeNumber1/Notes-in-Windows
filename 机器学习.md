## 机器学习西瓜书

#### 第一章-绪论

机器学习提供数据分析能力，云计算提供数据处理能力，众包提供数据标记能力

机器学习领域和数据库领域是数据挖掘的两大支撑

自动驾驶汽车中，机器学习起到了“司机”的作用

#### 第二章-模型评估与选择

**经验误差与过拟合**

误差均指误差期望

经验误差：学习器在训练集上的误差

泛化误差：在新样本上的误差

过拟合是机器学习面临的关键障碍，无法彻底避免，能做的只是缓解或减少其风险。

**评估方法**

使用一个`测试集`来测试学习器对新样本的判别能力，然后以测试集上的`测试误差`作为泛化误差的近似。

1. 留出法：直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T

2. 交叉验证法

   k=样本数量时，即特例，为留一法，但计算复杂度比较高

3. 自助法

   以自助采样法（有放回采样）为基础

   在数据集较、难以有效划分训练集/测试集时很有用

   产生的数据集改变了初始数据集的分布，会引入估计偏差

4. 调参与最终模型

   要注意，通常把学得模型在实际使用中遇到的数据称为`测试数据`，为了加以区分，模型评估与选择中用于评估测试的数据集常称为`验证集`。

   :question: 我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。

**性能度量**

1. 错误率与精度

2. 查准率、查全率与F1

   查准率和查全率是一对矛盾的度量，查准率高时，查全率往往偏低。

   P-R曲线：以查准率为纵轴，查全率为横轴作图，就得到查准率-查全率曲线

   ​	若一个学习器的P-R曲线被另一个学习器的曲线完全”包住“，则说明不如后者优越。

   ​	若两个学习器的曲线发生了交叉，则一般很难断言两者孰优孰劣

   综合考虑查准率、查全率的性能度量

   ​	1.平衡点（BEP）

   ​	2.F1度量

## 吴恩达机器学习

#### 第一章

**监督学习**

给算法一个数据集，其中包含了”正确答案“

1. 回归问题

​	目标是预测一个连续值输出

2. 分类问题（两类、多类）

​	预测离散值输出

如何处理无穷多个特征/属性

**无监督学习**

给出的数据没有任何标签

1. 聚类算法

​	簇（cluster）

#### 第二章

**模型描述**

线性回归(linear regression)

向学习算法提供训练集，输出一个函数，通常用小写h表示`假设函数`

**代价函数**

平方误差代价函数（用于回归问题）

$minimize(\theta_0, \theta_1)$ $J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}{(h(x) - y)}^2$

**梯度下降**

$\theta_j := \theta_j - \alpha \frac{\partial(\theta_0, \theta_1)}{\partial\theta_j}$

$\alpha​$过小会导致收敛缓慢，过大会导致不

当到达局部最低点时，梯度下降相当于什么都没做，于我们的预期相符

当我们接近局部最低点时，导数值会越来越小，梯度下降将自动采取较小的幅度

可以尝试用它最小化任何代价函数，而不仅是线性回归中的代价函数 

**梯度下降应用到线性回归问题上---线性回归的梯度下降**

Batch梯度下降法

​	每一次梯度下降，都遍历了整个训练集的样本

另一种求解代价函数J最小值的方法：正规方程组法，之后介绍

## 线性代数

**矩阵和向量**

**加法和标量乘法**

**矩阵向量乘法**

​	解决预测问题，一行代码，让数据矩阵和参数矩阵相乘

**矩阵乘法**

可以将大量的运算打包成一次矩阵乘法

**矩阵乘法特征**

​	不满足交换律

​	满足结合律 $A\times (B \times C) = (A\times B) \times C$

​	单位矩阵相当于实数范围内1的作用

**矩阵的逆**inverse

​	方阵才有逆矩阵

​	$AA^{-1} = A^{-1}A =  I$

​	如果A中所有元素为0，则其没有逆矩阵

​	没有逆矩阵的矩阵也称为`奇异矩阵`或`退化矩阵`（可看成非常接近于0）

**矩阵的转置**transpose

$A^T$

#### 多变量线性回归

**多特征**

特征向量

​	额外定义一个$x_0 = 1$，则$h(x) = \theta x_0 + \theta x_1 + ... + \theta x_n$，，进一步可以写为

​	$h(x) = \theta ^Tx$

符号定义

​	:boom:$x^{(i)}$为第i个样本

​	$x_1​$为第一个特征

**多元梯度下降法**

**多元梯度下降法演练I：特征缩放**

特征值的范围要接近，这样收敛过程会变快

并不需要太精确，只是为了让梯度下降运行的更快一点

1. 除以范围

2. 均值归一化

   $x1 = \frac{x_1 - \mu_1}{s1}，\mu_1为均值，s1为范围（或标准差）​$

**多元梯度下降法演练II：学习率**

确定梯度下降正确运行：

1. 通过绘制$J(\theta)$随迭代步数变化的曲线图观察，迭代之后代价函数变小(倾向于)

   如果曲线是上升的，往往可以通过减小学习率$\alpha$来实现

   如果曲线循环往复上升下降，也可以减小学习率

2. 自动收敛测试

   如果一步迭代后的下降小于1e-3，则说明已经收敛

学习率太小时，收敛(convergence)会很慢

学习率太大时，$J(\theta)$可能不会收敛

​	尝试不同的$\alpha$值：0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1

**特征和多项式回归**

定义新特征有时候可能得到更好的模型

如何将一个多项式（如一个二次函数/三次函数）拟合到我们的数据上

​	$h_0(x) = \theta_0 + \theta_1(size) + \theta_2(size)^2 + \theta_3(size)^3$

​	令$x_1 = (size), x_2 = (size)^2, x_3=(size)^3$

​	则$h_0(x) = \theta _0 + \theta x_1 + \theta x_2 + \theta x_3$

**正规方程**

$\theta = (X^{T}X)^{-1}X^{T}y​$

得到的 $\theta$ 即为使得代价函数最小的值

使用正规方程求解时可以不必进行特征缩放

**梯度下降与正规方程的选择**

梯度下降法需要选择合适的学习速率 $\alpha​$ ，并且需要很多迭代，还要画图判断敛散性，而正规方程不需要。

然而当n很大时，正规方程的速度会很慢，求解$(X^{T}X)^{-1}​$的时间接近于$O(n^3)​$，因此当n小于10000时，可以选择正规方程求解，当大得多时，就要考虑梯度下降或者其他方法了

#### Octave/Matlab基本操作

ones(n)生成一个全1的n阶方阵

eye(n)生成单位矩阵

hist()作图

size(A): 返回一个矩阵，记录A的两个维度行和列

length(a)：计算向量的长度，或者计算矩阵中最长的一维

C = [A B]：连接A、B（左右）

C = [A; B]: 连接A、B（上下）

A(:)：返回一个列向量

A(:, 2)：返回第二列的全部元素，也可以作为左值，给第二列赋值

load xxx.dat 加载数据

save hello.mat v 储存数据

点`.`说明是针对矩阵中元素的运算

​	如A .* B、A.^2、1 ./ A（矩阵每个元素取倒数）

常用函数 v = [1; 2; 3]

​	log(v)、exp(v)、v + ones(3, 1)相当于v + 1、

​	`A'`：A的转置

​	val = max(a)、[val, ind] = max(a) （其中a是一个向量，否则会得到每一列的最大值）

​	rand(3) magic(3)

​	max(A, [], 1) max(A, [], 2)

​	max(max(A)) = max(A(:))（求解矩阵中的最大元素，先变为一个向量）	

​	sum(A, 1) sum(A, 2) prod(a)

​	flipud(A)使矩阵垂直翻转（一般来取单位矩阵的反对角线）

​	pinv(A)求矩阵的逆矩阵（伪逆矩阵）

​	imagsc(A) 可视化矩阵A

**可视化**

hold on; 在原图基础上继续作图

plot(t, y1, 'r') 第三个参数可选，指定颜色

xlabel('time') ylabel('value') legend('sin', 'cos') title('myplot')

subplot(1, 2, 1) 把图分成1*2的网格，当前使用第一个

保存为图片：print -dpng 'myplot.png'

#### 所遇到相关知识

**西瓜书**

独立同分布

概率密度函数

**吴恩达**

什么样的矩阵有逆矩阵

偏导